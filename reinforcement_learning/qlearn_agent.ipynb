{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding the QLearningAgent: Simple vs. Neural RL\n",
        "\n",
        "This notebook explores the `QLearningAgent` class defined in `qlearn_agent.py`. This agent implements Q-learning, a fundamental Reinforcement Learning (RL) algorithm, and offers two modes of operation:\n",
        "\n",
        "1.  **`simple` mode:** Uses a traditional Q-table (a dictionary) to store Q-values. Suitable for problems with small, discrete state spaces.\n",
        "2.  **`neural` mode:** Uses a Deep Q-Network (DQN) (a neural network) to approximate Q-values. Necessary for problems with large or continuous state spaces.\n",
        "\n",
        "We will cover:\n",
        "*   Basic concepts of Reinforcement Learning and Q-Learning.\n",
        "*   The difference between the 'simple' (tabular) and 'neural' (DQN) approaches.\n",
        "*   Key components of the `QLearningAgent` class.\n",
        "*   A practical example demonstrating how to define states, actions, rewards, and use the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow Diagram\n",
        "\n",
        "![QLearningAgent Workflow](./dual_ql_agent.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Reinforcement Learning (RL) Basics\n",
        "\n",
        "Reinforcement Learning is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**. The goal is for the agent to learn a **policy** (a strategy) that maximizes a cumulative **reward** over time.\n",
        "\n",
        "Key components:\n",
        "*   **Agent:** The learner or decision-maker (e.g., our `QLearningAgent`).\n",
        "*   **Environment:** The external system the agent interacts with (e.g., a game, a simulation, the real world).\n",
        "*   **State (s):** A representation of the current situation of the environment.\n",
        "*   **Action (a):** A choice the agent can make in a given state.\n",
        "*   **Reward (r):** A scalar feedback signal from the environment indicating how good the last action was in the previous state.\n",
        "*   **Policy (π):** The agent's strategy, mapping states to actions.\n",
        "*   **Value Function (e.g., Q-value):** Predicts the expected future reward an agent can get by taking a specific action in a specific state and following a certain policy thereafter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Q-Learning: Learning Action Values\n",
        "\n",
        "Q-Learning is a popular RL algorithm that learns from experience without needing a model of the environment.\n",
        "\n",
        "### The Basic Setup:\n",
        "\n",
        "1. **State (s)**: What situation the agent is in right now (e.g., \"I'm in the top-left corner of a maze\").\n",
        "\n",
        "2. **Action (a)**: A choice the agent can make (e.g., \"move right,\" \"move down\").\n",
        "\n",
        "3. **Reward (r)**: Immediate feedback for taking an action in a state (e.g., \"+1 for getting closer to the goal,\" \"-1 for bumping into a wall\").\n",
        "\n",
        "The agent's goal is to pick actions that, over time, give it the most total reward.\n",
        "\n",
        "### The Q-Table\n",
        "\n",
        "Q-learning keeps a table of numbers called Q-values, one entry for each state-action pair, written as Q(s,a).\n",
        "\n",
        "- High Q(s,a) means \"I think doing action a in state s will pay off well.\"\n",
        "- Low Q(s,a) means \"That action probably won't help much (or might hurt).\"\n",
        "- Initially, you don't know anything, so you might start with all Q(s,a)=0.\n",
        "\n",
        "### Learning by Trial and Error\n",
        "\n",
        "1. Pick an action in the current state, sometimes at random (exploration) and sometimes the best-known one (exploitation).\n",
        "2. Take the action, observe the reward r and the new state s'.\n",
        "3. Update the Q-value for (s,a) to account for what you just learned.\n",
        "\n",
        "The core update formula is:\n",
        "\n",
        "$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha [ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) ] $$\n",
        "\n",
        "Where:\n",
        "- α (alpha) is the \"learning rate\" (how strongly new experiences override old knowledge).\n",
        "- γ (gamma) is the \"discount factor\" (how much you care about future rewards versus immediate ones).\n",
        "- $\\max_{a'} Q(s', a')$ is the best Q-value you think you can get from the new state s'.\n",
        "\n",
        "### Intuition Behind the Update\n",
        "\n",
        "You look at the difference between:\n",
        "- What you actually got (r + best future Q)\n",
        "- What you expected (old Q(s,a))\n",
        "\n",
        "Multiply that \"surprise\" by α, and add it into your old estimate. Over many trials, Q(s,a) converges toward the true long-term value of taking a in s.\n",
        "\n",
        "### What Happens Over Time\n",
        "\n",
        "- **At first**: The agent explores randomly, and Q-values jump around.\n",
        "- **As it gathers experience**: Q(s,a) estimates get better and better.\n",
        "- **Eventually**: The agent mostly picks the action with the highest Q(s,a) in each state, so it behaves optimally.\n",
        "\n",
        "### Simple Example: Maze Navigation\n",
        "\n",
        "- State = your current square in the maze.\n",
        "- Actions = up/down/left/right.\n",
        "- Reward = +10 for reaching the exit, -1 for each move (to encourage shorter paths).\n",
        "\n",
        "The agent tries random moves, updates Q, and gradually learns the fastest route to the exit.\n",
        "\n",
        "**Bottom line**: Q-learning is a straightforward way for an agent to learn how valuable each action is in each situation by continuously updating its estimates based on actual rewards received. Over time and repeated experience, it figures out which actions lead to the best long-term payoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. The `QLearningAgent`: Simple vs. Neural Mode\n",
        "\n",
        "Our `QLearningAgent` implements the Q-learning update logic. The key difference lies in how it stores and retrieves the \\(Q(s, a)\\) values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. `mode='simple'` (Tabular Q-Learning)\n",
        "\n",
        "*   **How it works:** Uses a Python dictionary (`self.q_table`) as a lookup table.\n",
        "    *   **Keys:** State representations (must be hashable, e.g., tuples).\n",
        "    *   **Values:** NumPy arrays where each element represents the Q-value for a specific action in that state. `q_table[state]` returns `[Q(state, action_0), Q(state, action_1), ...] `.\n",
        "*   **State Representation:** States must be discrete and hashable. For example, if a state is defined by `(position_x, position_y)`, the tuple `(5, 3)` can be a key.\n",
        "*   **Pros:**\n",
        "    *   Conceptually simple and easy to understand.\n",
        "    *   Guaranteed to converge to the optimal Q-values under certain conditions (enough exploration, appropriate learning rate decay).\n",
        "    *   Interpretable: You can directly inspect the learned Q-values for any state.\n",
        "*   **Cons:**\n",
        "    *   **Curse of Dimensionality:** Doesn't scale to problems with many state variables or continuous variables. The number of possible states (and thus the size of the Q-table) grows exponentially with the number of state features. Imagine a state with 10 features, each having 10 possible values -> 10^10 states!\n",
        "    *   No generalization: It learns values only for states it has explicitly visited. It cannot estimate Q-values for unseen but similar states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. `mode='neural'` (Deep Q-Network - DQN): Learning with a \"Brain\"\n",
        "\n",
        "When the number of possible situations (states) becomes enormous, or when states involve continuous values (like temperature or speed), the simple table approach (`mode='simple'`) breaks down. Imagine trying to create a table entry for every possible chess board position – it's impossible! This is called the \"curse of dimensionality.\"\n",
        "\n",
        "To overcome this, we use a **Deep Q-Network (DQN)**. Think of it like giving our agent a small \"brain\" – a neural network – instead of just a lookup table.\n",
        "\n",
        "*   **How it works:** Instead of storing a Q-value for every single state-action pair, the neural network (`self.model`) learns a *function* that *estimates* the Q-value.\n",
        "    *   **The Goal:** We want the network to learn the ideal Q-function, often called $Q^*(s, a)$, which tells us the true long-term value of taking action `a` in state `s`.\n",
        "    *   **The Approximation:** The network learns an *approximation* of this ideal function, represented as $Q(s, a; \\theta)$. Here, $\\theta$ (theta) stands for all the adjustable parameters (weights and biases) inside the network – the things the network \"learns\" by adjusting during training. So, the network aims to make $Q(s, a; \\theta)$ as close as possible to $Q^*(s, a)$.\n",
        "    *   **Input:** You feed the network a description of the current state `s`, but it must be converted into a list of numbers (a numerical vector). For example, instead of \"sunny day\", you might input `[1, 0, 25.5]` representing weather type and temperature.\n",
        "    *   **Output:** The network outputs a list (vector) of estimated Q-values, one for each possible action the agent can take from that state. For example, if the agent can go `left`, `right`, or `stay`, the output might look like `[10.2, -5.1, 1.5]`, meaning the network estimates the value of going left as 10.2, right as -5.1, and staying as 1.5. The agent would typically choose the action with the highest estimated Q-value (in this case, `left`).\n",
        "\n",
        "*   **State Representation (Turning Situations into Numbers):** Neural networks only understand numbers. So, any description of the state (text, images, categories, measurements) must be converted into a fixed-size list (vector) of numbers. This process is handled by the `_preprocess_state` function (which often needs customization for specific problems).\n",
        "    *   This allows us to handle complex states:\n",
        "        *   **Continuous values:** Like temperature (25.5°C) or speed (60.3 mph) can be used directly (perhaps after scaling them to a standard range like 0 to 1).\n",
        "        *   **High-dimensional discrete features:** Like words in a sentence or pixels in an image. Techniques like *embeddings* (representing words/items as dense vectors capturing meaning) or *one-hot encoding* (representing categories like 'cat'/'dog'/'bird' as `[1,0,0]`, `[0,1,0]`, `[0,0,1]`) are used to create these numerical vectors.\n",
        "\n",
        "*   **Pros (Why use a Neural Network?):**\n",
        "    *   **Scalability (Handles Huge Problems):** The network learns general patterns. Its size (number of parameters $\\theta$) doesn't necessarily explode even if there are billions of possible states. It can handle problems where creating a Q-table would be impossible due to memory limitations.\n",
        "    *   **Generalization (Learning from Similarity):** The network can make intelligent guesses about situations it hasn't encountered before. If it learns that state A (e.g., temperature 20°C) leads to a good outcome, and it encounters state B (e.g., temperature 21°C), it can infer that state B might also be good because they are similar numerically. The simple table method can only learn about states it has explicitly visited.\n",
        "\n",
        "*   **Cons (Downsides and Challenges):**\n",
        "    *   **More Complex Setup:** Designing the neural network (choosing layers, number of neurons) and tuning its learning process (setting the learning rate, choosing an optimizer algorithm) requires more expertise and experimentation than the simple table.\n",
        "    *   **Training Instability:** Sometimes, the learning process for a neural network can be unstable – the Q-value estimates might jump around wildly or fail to converge. Advanced techniques like *Experience Replay* (learning from a shuffled buffer of past experiences instead of just the latest one) and *Target Networks* (using a separate, slower-updating network to stabilize the target values) are often needed, adding complexity (these are not implemented in this basic version).\n",
        "    *   **Less Interpretable (\"Black Box\"):** It's much harder to look inside the trained neural network and understand *exactly why* it's predicting a certain Q-value for a given state and action. With the Q-table, you could just look up the value.\n",
        "    *   **Needs More Data and Power:** Neural networks typically require significantly more experience (data points) to learn effectively compared to tabular methods. Training them also demands more computational power, often benefiting greatly from GPUs (Graphics Processing Units)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Key Components of `QLearningAgent`\n",
        "\n",
        "Let's look at some important methods in the `qlearn_agent.py` script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `__init__(...)`\n",
        "Initializes the agent. Key parameters:\n",
        "*   `alpha`: Learning rate (how quickly the agent adapts).\n",
        "*   `gamma`: Discount factor (preference for immediate vs. future rewards).\n",
        "*   `epsilon`: Exploration rate (probability of choosing a random action vs. the best-known action).\n",
        "*   `mode`: `'simple'` or `'neural'`.\n",
        "*   `state_dim`: **Required** for `'neural'` mode. Defines the number of features in the input vector to the neural network *after* preprocessing.\n",
        "*   `n_actions`: Number of possible actions the agent can take.\n",
        "*   `data_file`: Path for saving/loading episode/error data (and Q-table in simple mode).\n",
        "*   `model_file`: Path for saving/loading the neural network model (neural mode)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `load_data()` / `save_data()`\n",
        "These methods handle persistence.\n",
        "*   **Simple Mode:** Saves/loads the `q_table` dictionary, `episodes` count, and `error_list` to/from `data_file` using `pickle`.\n",
        "*   **Neural Mode:** Saves/loads the neural network (`self.model`) using Keras' `save_model`/`load_model` to/from `model_file`. It also saves/loads `episodes` and `error_list` to/from `data_file` using `pickle`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `_preprocess_state(state)` (Crucial for Neural Mode)\n",
        "This method is responsible for converting the raw state representation into a numerical NumPy array suitable for the neural network. **You will almost certainly need to customize this method based on your specific problem.**\n",
        "\n",
        "The current implementation assumes the input `state` is already a list or tuple of numbers and reshapes it into `(1, state_dim)`.\n",
        "\n",
        "**Example Customization:**\n",
        "Let's say our state for a chatbot response agent is represented by a dictionary:\n",
        "`state = {'user_sentiment': 0.8, 'message_length': 55, 'topic': 'booking'}`\n",
        "And possible topics are `['greeting', 'booking', 'support', 'other']`.\n",
        "Our desired `state_dim` needs to account for all features numerically.\n",
        "\n",
        "1.  **Numerical Features:**\n",
        "    *   `user_sentiment`: Already numerical (e.g., -1 to 1). Might need scaling if the range is very large. Let's assume it's fine. (1 feature)\n",
        "    *   `message_length`: Numerical. Let's scale it to be roughly between 0 and 1 (e.g., assuming max length is 200). Scaled length = `min(message_length / 200.0, 1.0)`. (1 feature)\n",
        "2.  **Categorical Features:**\n",
        "    *   `topic`: Needs conversion. **One-Hot Encoding** is common. 'booking' would become `[0, 1, 0, 0]`. (4 features)\n",
        "\n",
        "**Total `state_dim` = 1 (sentiment) + 1 (scaled length) + 4 (topic OHE) = 6**\n",
        "\n",
        "A customized `_preprocess_state` might look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np # Already imported, but good practice in cell\n",
        "import logging # Already imported\n",
        "\n",
        "def custom_preprocess_state(state_dict, state_dim_expected=6):\n",
        "    \"\"\"\n",
        "    Example preprocessing for a specific state dictionary.\n",
        "    Converts {'user_sentiment': float, 'message_length': int, 'topic': str}\n",
        "    into a NumPy array of shape (1, 6).\n",
        "    \"\"\"\n",
        "    topics = ['greeting', 'booking', 'support', 'other']\n",
        "    \n",
        "    try:\n",
        "        # 1. Extract and scale numerical features\n",
        "        sentiment = state_dict.get('user_sentiment', 0.0) # Default to neutral\n",
        "        \n",
        "        msg_len = state_dict.get('message_length', 0)\n",
        "        scaled_length = min(msg_len / 200.0, 1.0) # Example scaling\n",
        "        \n",
        "        # 2. One-Hot Encode categorical features\n",
        "        topic = state_dict.get('topic', 'other') # Default to 'other'\n",
        "        topic_vector = np.zeros(len(topics))\n",
        "        if topic in topics:\n",
        "            topic_index = topics.index(topic)\n",
        "            topic_vector[topic_index] = 1.0\n",
        "        else: # Handle unknown topic - maybe map to 'other'?\n",
        "             topic_index = topics.index('other')\n",
        "             topic_vector[topic_index] = 1.0\n",
        "            \n",
        "        # 3. Concatenate features into a single vector\n",
        "        feature_vector = np.concatenate(([sentiment, scaled_length], topic_vector))\n",
        "        \n",
        "        # 4. Reshape for Keras (batch size of 1)\n",
        "        processed_arr = feature_vector.reshape(1, -1)\n",
        "        \n",
        "        # 5. Validation\n",
        "        if processed_arr.shape[1] != state_dim_expected:\n",
        "            raise ValueError(f\"Processed state shape {processed_arr.shape} != expected ({1, state_dim_expected})\")\n",
        "            \n",
        "        return processed_arr.astype(np.float32)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing state {state_dict}: {e}\")\n",
        "        # Return zeros or handle error appropriately\n",
        "        return np.zeros((1, state_dim_expected), dtype=np.float32) \n",
        "\n",
        "# --- Test it ---\n",
        "example_state = {'user_sentiment': 0.8, 'message_length': 55, 'topic': 'booking'}\n",
        "processed = custom_preprocess_state(example_state)\n",
        "print(f\"Original state: {example_state}\")\n",
        "print(f\"Processed state shape: {processed.shape}\")\n",
        "print(f\"Processed state content: {processed}\")\n",
        "\n",
        "example_state_unknown = {'user_sentiment': -0.5, 'message_length': 300, 'topic': 'complaint'}\n",
        "processed_unknown = custom_preprocess_state(example_state_unknown)\n",
        "print(f\"\\nOriginal state: {example_state_unknown}\")\n",
        "print(f\"Processed state shape: {processed_unknown.shape}\")\n",
        "print(f\"Processed state content: {processed_unknown}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Note: In the actual agent, you would replace the body of `_preprocess_state` with this custom logic, ensuring `self.state_dim` matches the output.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `get_q_values(state)`\n",
        "Returns the Q-values for all actions in the given `state`.\n",
        "*   **Simple Mode:** Looks up `state` in `self.q_table`. If the state is new, it initializes Q-values (usually to zeros). Requires the state to be hashable (e.g., tuple).\n",
        "*   **Neural Mode:** Calls `_preprocess_state(state)` and then uses `self.model.predict()` (or `self.model()`) on the processed state to get the Q-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `update_q_value(...)` / `_train_step(...)`\n",
        "This is where learning happens based on a transition `(state, action, reward, next_state)`.\n",
        "*   **Simple Mode:** Directly applies the Q-learning update rule to modify the value in `self.q_table[state][action]`.\n",
        "*   **Neural Mode:**\n",
        "    1.  Calculates the TD Target: \\( \\text{target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta) \\). This requires predicting Q-values for the `next_state` using the network.\n",
        "    2.  Calls `_train_step(state_tensor, action_tensor, target_tensor)`.\n",
        "    3.  `_train_step` performs one step of gradient descent using the optimizer (`self.optimizer`). It calculates the loss (e.g., Mean Squared Error) between the network's prediction for the *action taken* (`Q(s, action; \\theta)`) and the calculated `target`. The network weights (\\(\\theta\\)) are updated to minimize this loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `choose_action(state)`\n",
        "Implements the **Epsilon-Greedy** strategy for exploration/exploitation:\n",
        "1.  Generate a random number between 0 and 1.\n",
        "2.  If the number is less than `self.epsilon`:\n",
        "    *   **Explore:** Choose a random action.\n",
        "3.  Otherwise (with probability `1 - epsilon`):\n",
        "    *   **Exploit:** Choose the action with the highest Q-value according to `get_q_values(state)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Practical Example: Simple Grid World Navigation\n",
        "\n",
        "Let's define a very simple environment: a 3x3 grid.\n",
        "*   **States:** Agent's position `(row, col)`, e.g., `(0, 0)` to `(2, 2)`.\n",
        "*   **Actions:** `0: Up, 1: Down, 2: Left, 3: Right`. (Total `n_actions = 4`).\n",
        "*   **Environment:**\n",
        "    *   Start at `(0, 0)`.\n",
        "    *   Goal at `(2, 2)`.\n",
        "    *   Obstacle at `(1, 1)`.\n",
        "    *   Hitting a wall or the obstacle keeps the agent in the same state.\n",
        "*   **Rewards:**\n",
        "    *   +10 for reaching the goal `(2, 2)`.\n",
        "    *   -10 for hitting the obstacle `(1, 1)`.\n",
        "    *   -1 for any other move (encourages shortest path).\n",
        "*   **Episode End:** Reaching the goal or the obstacle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define environment parameters\n",
        "GRID_SIZE = 3\n",
        "GOAL_STATE = (GRID_SIZE - 1, GRID_SIZE - 1) # (2, 2)\n",
        "START_STATE = (0, 0)\n",
        "OBSTACLE_STATE = (1, 1)\n",
        "N_ACTIONS = 4 # 0: Up, 1: Down, 2: Left, 3: Right\n",
        "\n",
        "# Simple environment step function\n",
        "def environment_step(state, action):\n",
        "    \"\"\"Simulates one step in the grid world.\"\"\"\n",
        "    row, col = state\n",
        "    \n",
        "    # Apply action\n",
        "    if action == 0: # Up\n",
        "        row = max(0, row - 1)\n",
        "    elif action == 1: # Down\n",
        "        row = min(GRID_SIZE - 1, row + 1)\n",
        "    elif action == 2: # Left\n",
        "        col = max(0, col - 1)\n",
        "    elif action == 3: # Right\n",
        "        col = min(GRID_SIZE - 1, col + 1)\n",
        "    \n",
        "    next_state = (row, col)\n",
        "    \n",
        "    # Check for obstacle collision AFTER movement attempt\n",
        "    # Simplified reward logic - This was refined in the original text, but let's keep the final version:\n",
        "    if next_state == GOAL_STATE:\n",
        "        reward = 10\n",
        "        done = True\n",
        "    elif next_state == OBSTACLE_STATE: # If move resulted in landing ON obstacle\n",
        "        reward = -10\n",
        "        done = True\n",
        "        # The original text discusses how to handle this (stay in previous state or move into obstacle state)\n",
        "        # This version allows moving into it and ending. \n",
        "        pass \n",
        "    else:\n",
        "        reward = -1 # Step cost\n",
        "        done = False\n",
        "        \n",
        "    return next_state, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Using `mode='simple'`\n",
        "\n",
        "Since the state space is small (3x3 = 9 states), the simple Q-table approach is perfect. The state `(row, col)` is already a tuple, which is hashable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume QLearningAgent class is defined in qlearn_agent.py and importable\n",
        "from qlearn_agent import QLearningAgent \n",
        "\n",
        "# Agent parameters\n",
        "ALPHA = 0.1 # Learning rate (lower for simple mode often works)\n",
        "GAMMA = 0.9 # Discount factor\n",
        "EPSILON = 1.0 # Initial exploration rate (start high)\n",
        "EPSILON_DECAY = 0.995 # Decay factor per episode\n",
        "EPSILON_MIN = 0.05 # Minimum exploration rate\n",
        "N_EPISODES = 1000 # Number of training episodes\n",
        "\n",
        "# Initialize agent\n",
        "agent_simple = QLearningAgent(\n",
        "    alpha=ALPHA, \n",
        "    gamma=GAMMA, \n",
        "    epsilon=EPSILON, \n",
        "    mode='simple', \n",
        "    n_actions=N_ACTIONS,\n",
        "    data_file=\"rl_agent/simple_q_data.pkl\" # Use separate file\n",
        ")\n",
        "print(\"Simple Agent Initialized\")\n",
        "\n",
        "# Training loop\n",
        "current_epsilon = EPSILON\n",
        "episode_rewards = []\n",
        "\n",
        "print(\"--- Training Simple Q-Learning Agent ---\")\n",
        "for episode in range(N_EPISODES):\n",
        "    state = START_STATE\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    \n",
        "    while not done and steps < 100: # Add max steps per episode\n",
        "        # Choose action using epsilon-greedy\n",
        "        action = agent_simple.choose_action(state)\n",
        "        \n",
        "        # Take action, observe outcome\n",
        "        next_state, reward, done = environment_step(state, action)\n",
        "        \n",
        "        # Update Q-value\n",
        "        _ , td_error = agent_simple.update_q_value(state, action, reward, next_state, use_next_state=not done)\n",
        "        \n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        \n",
        "    episode_rewards.append(total_reward)\n",
        "    \n",
        "    # Decay epsilon\n",
        "    current_epsilon = max(EPSILON_MIN, current_epsilon * EPSILON_DECAY)\n",
        "    agent_simple.epsilon = current_epsilon # Update agent's epsilon\n",
        "    \n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-100:])\n",
        "        print(f\"Episode {episode + 1}/{N_EPISODES} | Avg Reward (last 100): {avg_reward:.2f} | Epsilon: {current_epsilon:.3f}\")\n",
        "\n",
        "print(\"--- Training Finished ---\") \n",
        "\n",
        "# Inspect Q-values\n",
        "print(\"\\nQ-values for Start State (0, 0):\", agent_simple.get_q_values((0, 0)))\n",
        "print(\"Q-values for State (1, 0):\", agent_simple.get_q_values((1, 0))) \n",
        "print(\"Q-values for State (1, 2):\", agent_simple.get_q_values((1, 2))) \n",
        "\n",
        "# Testing Greedy Policy\n",
        "print(\"\\n--- Testing Greedy Policy ---\")\n",
        "state = START_STATE\n",
        "steps = 0\n",
        "path = [state]\n",
        "agent_simple.epsilon = 0.0 # Turn off exploration\n",
        "while state != GOAL_STATE and state != OBSTACLE_STATE and steps < 20:\n",
        "    action = agent_simple.choose_action(state) # Choose best action\n",
        "    state, reward, done = environment_step(state, action)\n",
        "    path.append(state)\n",
        "    steps += 1\n",
        "    if done:\n",
        "        break\n",
        "print(\"Path:\", path)\n",
        "print(\"Reached Goal?\", state == GOAL_STATE)\n",
        "print(\"Hit Obstacle?\", state == OBSTACLE_STATE)\n",
        "\n",
        "# Clean up\n",
        "data_file_path = \"rl_agent/simple_q_data.pkl\"\n",
        "if os.path.exists(data_file_path):\n",
        "    os.remove(data_file_path)\n",
        "    print(f\"Removed {data_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Using `mode='neural'`\n",
        "\n",
        "Although overkill for this tiny grid world, let's demonstrate how to set it up. We need to preprocess the state `(row, col)` into a numerical vector.\n",
        "\n",
        "**Preprocessing:** We can represent the state as a 2-element vector `[row, col]`. It's often good practice to scale these features, especially if the grid were larger. Let's scale them to `[0, 1]` by dividing by `(GRID_SIZE - 1)`.\n",
        "\n",
        "**`state_dim` will be 2.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the preprocessing function specifically for this grid world\n",
        "def preprocess_grid_state(state_tuple, grid_size):\n",
        "    \"\"\"Converts (row, col) tuple to a scaled NumPy array [row_scaled, col_scaled].\"\"\"\n",
        "    row, col = state_tuple\n",
        "    # Scale features to [0, 1]\n",
        "    row_scaled = row / (grid_size - 1.0)\n",
        "    col_scaled = col / (grid_size - 1.0)\n",
        "    \n",
        "    feature_vector = np.array([row_scaled, col_scaled], dtype=np.float32)\n",
        "    # Reshape for Keras: (1, state_dim)\n",
        "    return feature_vector.reshape(1, -1)\n",
        "\n",
        "# --- Test preprocessing ---\n",
        "test_state = (1, 2)\n",
        "state_dim = 2 # We have two features: row_scaled, col_scaled\n",
        "processed_test = preprocess_grid_state(test_state, GRID_SIZE)\n",
        "print(f\"Original: {test_state}, Processed: {processed_test}, Shape: {processed_test.shape}\")\n",
        "\n",
        "# Note on agent's internal preprocessing mentioned in the original text:\n",
        "# We would typically modify the agent's _preprocess_state or pass the preprocessor.\n",
        "# For this example, we'll preprocess *before* calling agent methods OR monkey-patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agent parameters for Neural mode (might need different tuning)\n",
        "ALPHA_NEURAL = 0.001 # Learning rate for Adam optimizer (often smaller for NNs)\n",
        "GAMMA_NEURAL = 0.9 \n",
        "EPSILON_NEURAL = 1.0 \n",
        "EPSILON_DECAY_NEURAL = 0.998 # May need slower decay\n",
        "EPSILON_MIN_NEURAL = 0.05\n",
        "N_EPISODES_NEURAL = 2000 # May need more episodes for NN to converge\n",
        "\n",
        "# State dimension after preprocessing\n",
        "STATE_DIM_GRID = 2 \n",
        "\n",
        "# --- Initialize agent (Placeholder) --- \n",
        "# This requires the QLearningAgent class definition and TensorFlow\n",
        "# Make sure TensorFlow is installed: pip install tensorflow\n",
        "agent_neural = QLearningAgent(\n",
        "    alpha=ALPHA_NEURAL, # Used by Adam optimizer if optimizer state not loaded\n",
        "    gamma=GAMMA_NEURAL, \n",
        "    epsilon=EPSILON_NEURAL, \n",
        "    mode='neural', \n",
        "    state_dim=STATE_DIM_GRID, \n",
        "    n_actions=N_ACTIONS,\n",
        "    data_file=\"rl_agent/neural_q_data.pkl\", # Separate common data file\n",
        "    model_file=\"rl_agent/neural_q_model.h5\"  # Separate model file\n",
        ")\n",
        "print(\"Neural Agent Initialized (Placeholder - requires QLearningAgent class)\")\n",
        "\n",
        "# !! Important: Agent needs to use the custom preprocessor !!\n",
        "# Option 1: Modify the class (recommended)\n",
        "# Option 2: Monkey-patch the instance (quick demo)\n",
        "# Option 3: Preprocess manually before every call (verbose)\n",
        "\n",
        "# --- Training loop (Placeholder) --- \n",
        "# This loop needs the 'agent_neural' object and the monkey-patch or class modification\n",
        "current_epsilon = EPSILON_NEURAL\n",
        "episode_rewards_neural = []\n",
        "episode_losses = [] # Track NN loss\n",
        "\n",
        "print(\"\\n--- Training Neural (DQN) Agent (Simulation - requires Agent) ---\")\n",
        "# TEMPORARY MONKEY-PATCH (Example - Apply to actual agent object if created)\n",
        "# This replaces the method ONLY for the 'agent_neural' instance\n",
        "if 'agent_neural' in locals(): # Check if agent was created\n",
        "     agent_neural._preprocess_state = lambda state: preprocess_grid_state(state, GRID_SIZE)\n",
        "     print(\"Agent's _preprocess_state monkey-patched.\")\n",
        "else:\n",
        "     print(\"Agent object not created, cannot monkey-patch.\")\n",
        "\n",
        "for episode in range(N_EPISODES_NEURAL):\n",
        "    state_tuple = START_STATE\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "    \n",
        "    while not done and steps < 100: \n",
        "        # Action choice (relies on patched _preprocess_state)\n",
        "        action = agent_neural.choose_action(state_tuple) \n",
        "        \n",
        "        # Environment step\n",
        "        next_state_tuple, reward, done = environment_step(state_tuple, action)\n",
        "        \n",
        "        # Update / Train (relies on patched _preprocess_state)\n",
        "        loss, td_error = agent_neural.update_q_value(\n",
        "            state_tuple, \n",
        "            action, \n",
        "            reward, \n",
        "            next_state_tuple, \n",
        "            use_next_state=not done\n",
        "        )\n",
        "        \n",
        "        if not np.isnan(loss):\n",
        "             total_loss += loss\n",
        "            \n",
        "        state_tuple = next_state_tuple\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        \n",
        "    episode_rewards_neural.append(total_reward)\n",
        "    avg_loss = total_loss / steps if steps > 0 else 0\n",
        "    episode_losses.append(avg_loss)\n",
        "    \n",
        "    # Decay epsilon\n",
        "    current_epsilon = max(EPSILON_MIN_NEURAL, current_epsilon * EPSILON_DECAY_NEURAL)\n",
        "    agent_neural.epsilon = current_epsilon # Update agent's epsilon\n",
        "    \n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg_reward = np.mean(episode_rewards_neural[-100:])\n",
        "        avg_loss_report = np.mean(episode_losses[-100:])\n",
        "        print(f\"Episode {episode + 1}/{N_EPISODES_NEURAL} | Avg Reward: {avg_reward:.2f} | Avg Loss: {avg_loss_report:.4f} | Epsilon: {current_epsilon:.3f}\")\n",
        "\n",
        "print(\"--- Training Finished (Simulation) ---\")\n",
        "\n",
        "# --- Testing Greedy Policy (Placeholder) --- \n",
        "print(\"\\n--- Testing Greedy Policy (Neural - Simulation) ---\")\n",
        "state_tuple = START_STATE\n",
        "steps = 0\n",
        "path = [state_tuple]\n",
        "agent_neural.epsilon = 0.0 # Turn off exploration\n",
        "while state_tuple != GOAL_STATE and state_tuple != OBSTACLE_STATE and steps < 20:\n",
        "    # Use the patched preprocessor implicitly via choose_action\n",
        "    action = agent_neural.choose_action(state_tuple) \n",
        "    state_tuple, reward, done = environment_step(state_tuple, action)\n",
        "    path.append(state_tuple)\n",
        "    steps += 1\n",
        "    if done:\n",
        "        break\n",
        "print(\"Path:\", path)\n",
        "print(\"Reached Goal?\", state_tuple == GOAL_STATE)\n",
        "print(\"Hit Obstacle?\", state_tuple == OBSTACLE_STATE)\n",
        "\n",
        "# --- Clean up (Placeholder) --- \n",
        "data_file_path = \"rl_agent/neural_q_data.pkl\"\n",
        "model_file_path = \"rl_agent/neural_q_model.h5\"\n",
        "if os.path.exists(data_file_path):\n",
        "    os.remove(data_file_path)\n",
        "    print(f\"Removed {data_file_path}\")\n",
        "if os.path.exists(model_file_path):\n",
        "    os.remove(model_file_path)\n",
        "    print(f\"Removed {model_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusion & Next Steps\n",
        "\n",
        "This notebook demonstrated the `QLearningAgent` which supports both tabular ('simple') and neural network-based ('neural') Q-learning.\n",
        "\n",
        "*   **Simple Mode:** Effective and interpretable for small, discrete state spaces. Limited by the curse of dimensionality.\n",
        "*   **Neural Mode (DQN):** Can handle large/continuous state spaces through function approximation and generalize to unseen states. Requires careful state preprocessing and potentially more complex training techniques.\n",
        "\n",
        "**Key Takeaways:**\n",
        "*   The choice between modes depends heavily on the nature and size of your problem's state space.\n",
        "*   State representation and preprocessing are critical, especially for DQN. Features should be numerical, and often scaled or encoded appropriately (e.g., one-hot encoding for categoricals).\n",
        "*   Hyperparameter tuning (alpha, gamma, epsilon, network architecture, optimizer learning rate) is crucial for good performance.\n",
        "\n",
        "**Potential Improvements (especially for DQN):**\n",
        "*   **Experience Replay:** Store transitions `(s, a, r, s')` in a buffer and sample mini-batches randomly for training. This breaks correlations between consecutive samples and improves stability.\n",
        "*   **Target Network:** Use a separate, periodically updated copy of the main network to calculate the TD target values. This further stabilizes training by reducing the chasing of a moving target.\n",
        "*   **Epsilon Decay:** Implement a more sophisticated epsilon decay schedule.\n",
        "*   **More Complex Environments:** Try applying the agent to more challenging problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Core Agent Usage: Initialization, Getting Q-Values, and Updating\n",
        "\n",
        "Let's look at the fundamental operations of the `QLearningAgent`:\n",
        "\n",
        "1.  **Initialization (`__init__`)**: Creates the agent. Key parameters include:\n",
        "    *   `mode`: 'simple' or 'neural'.\n",
        "    *   `state_dim` (required for 'neural'): The number of features in your state representation.\n",
        "    *   `n_actions`: The total number of possible actions the agent can take.\n",
        "    *   `alpha`, `gamma`, `epsilon`: Learning rate, discount factor, and exploration rate.\n",
        "    *   `data_file`, `model_file`: Paths for saving/loading agent state and the neural model.\n",
        "\n",
        "2.  **Getting Q-Values (`get_q_values`)**: Given a state, this method returns the predicted Q-values for all possible actions in that state.\n",
        "    *   In 'simple' mode, it looks up the state in the Q-table (or returns defaults if the state is new).\n",
        "    *   In 'neural' mode, it feeds the state representation into the neural network and gets the output layer's activations (representing Q-values for each action).\n",
        "\n",
        "3.  **Updating Q-Values (`update_q_value`)**: This is the core learning step. It takes the experience tuple `(state, action, reward, next_state)` and updates the agent's knowledge.\n",
        "    *   In 'simple' mode, it applies the Q-learning update rule directly to the Q-table entry for `(state, action)`.\n",
        "    *   In 'neural' mode, it calculates the target Q-value using the reward and the maximum predicted Q-value for the `next_state`, then performs a gradient descent step (using the optimizer like Adam) to adjust the network's weights to minimize the difference (Temporal Difference error) between the predicted Q-value for `(state, action)` and the target value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example demonstrating core agent usage (Neural Mode)\n",
        "import numpy as np\n",
        "\n",
        "# Assuming qlearn_agent.py is in the parent directory or accessible via PYTHONPATH\n",
        "# If running directly from the directory containing reinforcement_learning, adjust path:\n",
        "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
        "from reinforcement_learning import QLearningAgent # Make sure this import works in your environment\n",
        "\n",
        "# --- Parameters (matching RlMetaRag context) ---\n",
        "STATE_DIM = 10 # Example: 9 features + query length\n",
        "N_ACTIONS = 3  # Example: 3 different RAG techniques\n",
        "ALPHA = 0.01   # Learning rate for the optimizer\n",
        "GAMMA = 0.95   # Discount factor\n",
        "EPSILON = 0.1  # Exploration rate (less relevant for direct update demo)\n",
        "DATA_FILE = \"./temp_neural_data.pkl\"\n",
        "MODEL_FILE = \"./temp_neural_model.h5\"\n",
        "\n",
        "# --- Sample Data ---\n",
        "# Represent state as a numpy array (e.g., extracted query features)\n",
        "sample_state = np.array([0, 1, 1, 0.5, 0.2, 15, 0.8, 0.1, 0.0, 15], dtype=np.float32)\n",
        "sample_action = 1 # Agent chose the second RAG technique (index 1)\n",
        "sample_reward = 0.8 # Received a positive reward (e.g., user rated the result as good)\n",
        "# Next state could be the features of a subsequent query, or None if terminal\n",
        "# For this example, let's assume a non-terminal step with a new state\n",
        "sample_next_state = np.array([1, 2, 0, 0.8, 0.1, 25, 0.9, 0.5, 0.2, 25], dtype=np.float32)\n",
        "\n",
        "# --- Agent Initialization ---\n",
        "print(f\"Initializing agent in neural mode (State Dim: {STATE_DIM}, Actions: {N_ACTIONS})\")\n",
        "agent = QLearningAgent(\n",
        "    mode='neural',\n",
        "    state_dim=STATE_DIM,\n",
        "    n_actions=N_ACTIONS,\n",
        "    alpha=ALPHA,\n",
        "    gamma=GAMMA,\n",
        "    epsilon=EPSILON,\n",
        "    data_file=DATA_FILE,\n",
        "    model_file=MODEL_FILE\n",
        ")\n",
        "\n",
        "# --- Get Initial Q-Values ---\n",
        "initial_q_values = agent.get_q_values(sample_state)\n",
        "\n",
        "# --- Update Q-Value based on experience ---\n",
        "agent.update_q_value(sample_state, sample_action, sample_reward, sample_next_state)\n",
        "\n",
        "\n",
        "# --- Get Q-Values After Update ---\n",
        "# Note: A single update might only slightly change the NN weights.\n",
        "# Significant changes require more training steps/epochs.\n",
        "updated_q_values = agent.get_q_values(sample_state)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
